{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ea499-735d-4a1f-a29a-dbb2c3f9fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85876a-bfb2-4fd0-958a-6d3431a0b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d056a64-d287-4ee1-833b-7b72e8139514",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "We first need to prepare the data set: download the data, split train/val set and do normalisation. This part is taken from https://teaching.pages.centralesupelec.fr/deeplearning-lectures-build/00-pytorch-fashionMnist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c2a4d-b0ce-41c1-ab6a-34cdf396079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the data\n",
    "# Change path accordingly\n",
    "dataset_dir = \"/home/mfauvel/Documents/Data/FashionMNIST\"\n",
    "\n",
    "train_valid_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=dataset_dir, train=True, transform=None, download=True\n",
    ")\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(\n",
    "    train_valid_dataset, [0.8, 0.2]\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=dataset_dir, transform=None, train=False  # transforms.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1bc0a-2e0a-42d8-a48c-b158ffaaefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute the normalisation\n",
    "ToTensor = transforms.ToTensor()  # Transform pil image to pytorch Tensor\n",
    "mean_ = torch.zeros_like(ToTensor(train_dataset[0][0]))\n",
    "std_ = torch.zeros_like(ToTensor(train_dataset[0][0]))\n",
    "for img, _ in train_dataset:\n",
    "    mean_ += ToTensor(img)\n",
    "mean_ /= len(train_dataset)\n",
    "for img, _ in train_dataset:\n",
    "    std_ += (mean_ - ToTensor(img)) ** 2\n",
    "std_ /= len(train_dataset)\n",
    "std_ = torch.sqrt(std_)\n",
    "std_[std_ == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9002f7b-5043-438d-ba6c-a824fe37b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare pytorch dataset\n",
    "class DatasetTransformer(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, transform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.base_dataset[index]\n",
    "        return self.transform(img), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Lambda(lambda x: (x - mean_) / std_)]\n",
    ")\n",
    "train_dataset = DatasetTransformer(train_dataset, data_transforms)\n",
    "valid_dataset = DatasetTransformer(valid_dataset, data_transforms)\n",
    "test_dataset = DatasetTransformer(test_dataset, data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78e34c-f357-4d60-9bb6-a4d8b66a7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare data loader\n",
    "num_threads = 4  # Loading the dataset is using 4 CPU threads\n",
    "batch_size = 512  # Using minibatches of 512 samples\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_threads\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_threads\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_threads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95779dbe-1977-4052-8896-bbe25499954b",
   "metadata": {},
   "source": [
    "We can plot data of the train set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b3d98-2326-4b34-b232-5a5c2080b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 10\n",
    "classes_names = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "imgs, labels = next(iter(train_loader))\n",
    "imgs = imgs * std_ + mean_  # Get back of the normalization\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5), facecolor=\"w\")\n",
    "for i in range(nsamples):\n",
    "    ax = plt.subplot(1, nsamples, i + 1)\n",
    "    plt.imshow(imgs[i, 0, :, :], vmin=0, vmax=1.0, cmap=cm.gray)\n",
    "    ax.set_title(\"{}\".format(classes_names[labels[i]]), fontsize=15)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982b427-2fa0-4ad8-924a-1969c841be59",
   "metadata": {},
   "source": [
    "We have now the data set ready. We can now define the model and the optimization model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f248b-d395-429f-af50-155a4ff47061",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdeb8d-2717-423d-899e-18f38f410c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassificationModel(nn.Module):\n",
    "    def __init__(self, n_features: int, n_classes: int):\n",
    "        \"\"\"n_feature is the number of pixel of the images: n_features = W*H\"\"\"\n",
    "        super(LinearClassificationModel, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X: torch.Tensor)-> torch.Tensor:\n",
    "        pass\n",
    "        \n",
    "    def predict_proba(self, X: torch.Tensor)-> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def predict(self, X: torch.Tensor)-> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66854520-c4b9-448b-9a7d-a17de0ac9944",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5e13e-0139-4de3-a0a0-d8044218393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "learning_rate = 0.0001\n",
    "model = LinearClassificationModel(28 * 28, 10)\n",
    "loss_fn = nn.CrossEntropyLoss() # Read the documentation : https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_set_len = len(train_loader)\n",
    "val_set_len = len(valid_loader)\n",
    "train_loss, val_loss = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    accu = 0.0\n",
    "\n",
    "    for X_, y_ in train_loader:\n",
    "        # Forward pass\n",
    "        y_hat = model(X_)\n",
    "        loss = loss_fn(y_hat, y_)\n",
    "        accu += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss.append(accu / train_set_len)\n",
    "\n",
    "    # Validation - no gradient & eval mode\n",
    "    model.eval()\n",
    "    accu = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_, y_ in valid_loader:\n",
    "            # Forward pass\n",
    "            y_hat = model(X_)\n",
    "            loss = loss_fn(y_hat, y_)\n",
    "            accu += loss.item()\n",
    "        val_loss.append(accu / val_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01607136-022e-4cc2-ad16-d03d39333849",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11a868-00e2-498e-bab7-7911679e8013",
   "metadata": {},
   "source": [
    "# To Do\n",
    "- Compute the classification accuracy with the model\n",
    "- Take a wrongly classified image, and look at the probability membership for each class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
