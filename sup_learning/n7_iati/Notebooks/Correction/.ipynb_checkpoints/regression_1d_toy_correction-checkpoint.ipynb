{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656cae71-ae4b-4615-9954-06e7bbc5100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bee9ae-89fe-44eb-841d-152d30100971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5852be-1fdf-453e-ad90-55ff8dd7352e",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcbf28-1938-4da5-a840-cb72fb19c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the function to be estimated\n",
    "def function_to_be_estimated(\n",
    "    x: torch.Tensor, random_seed: int = 0, noise: bool = True, noise_level: float = 5\n",
    "):\n",
    "    torch.manual_seed(random_seed)\n",
    "    y = 2 * x - 1\n",
    "    if noise:\n",
    "        y += noise_level * torch.randn(x.shape[0], x.shape[1])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f9600-797e-4772-85df-9d6db2e5268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample generation\n",
    "torch.manual_seed(0)\n",
    "n_samples = 50\n",
    "x_train = 20 * torch.rand(n_samples, 1) - 5\n",
    "y_train = function_to_be_estimated(x_train)\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.grid(\"on\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1ff97-52ec-4afe-91d0-e92d8bbb35ff",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae31cfa-be87-4b5c-a7f5-098d787601e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear_layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039aff12-c589-403f-8783-aa6a54f6001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and print initial values (random init)\n",
    "model = LinearModel()\n",
    "print(model.linear_layer.weight)\n",
    "print(model.linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e6aa6-2e46-4c1a-aad3-56236f5d5ccb",
   "metadata": {},
   "source": [
    "# Learn the model with observations\n",
    "## Gradient descent\n",
    "We have some data and the model. To learn the model parameter we need two additional ingredients: the loss function and the optimizer. In this simple case, we will the square error and the objective function will be the mean square error. For the optimizer, we will use a simple gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3d664-1b69-4b89-a964-df7fe095d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b3a62-7806-4a1e-9ea6-44d7d7407cd5",
   "metadata": {},
   "source": [
    "We can iterate gradient update until convergence and make use of autograd to let pytorch computes the derivative by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b735b6f-c681-40dc-915a-3c1ac3682b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_val = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_hat = model(x_train)\n",
    "\n",
    "    # Compute objective\n",
    "    loss = loss_fn(y_train, y_hat)\n",
    "    loss_val.append(loss.item())\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\" Epoch {epoch}: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6b571-3694-4b4f-9484-de7df5c579d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_val)\n",
    "plt.grid(\"on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b54457-46ac-4647-a907-f5d0844ffa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.linear_layer.weight)\n",
    "print(model.linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3e47c-54af-473a-b374-5e49ecef4be1",
   "metadata": {},
   "source": [
    "### Plot the final estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb6b94-2fbb-4001-b915-67676e415c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x = torch.linspace(-5, 15, 1000)\n",
    "y_true = function_to_be_estimated(x, noise=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(torch.unsqueeze(x, 1))\n",
    "plt.plot(x, y_true, \"k\")\n",
    "plt.plot(x, y_hat.detach().numpy(), \"r\")\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.grid(\"on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0837a-7f2d-4a86-a89f-aaed53d6ee47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Some points to investigate before going further\n",
    "- Change the learning rate of the optimizer to see how it changes the convergence.\n",
    "- Change to batch gradient update, using pytorch dataloader (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "- Try to improve your fit:\n",
    "    - Increase the number of training samples: what does it change in the final result ? \n",
    "    - Add another linear layer in your model: what does it change in the final result ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25c795-6c2b-481f-8a8d-e3b897e3cd84",
   "metadata": {},
   "source": [
    "# Visualizing convergence - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43d7af-598c-4df9-b4e2-86fda52716ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    model: LinearModel,\n",
    "    loss_fn: nn.MSELoss,\n",
    "    lr: float = 0.001,\n",
    "):\n",
    "    \"\"\"helper function to plot gradient update w.r.t the loss landscape\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression as LR\n",
    "\n",
    "    assert (\n",
    "        len(list(model.parameters())) == 2\n",
    "    ), \"This code works only for a linear 1D model\"\n",
    "\n",
    "    # Plot the error function\n",
    "    n_w, n_b = 100, 100\n",
    "    W = torch.linspace(-1, 4, n_w)\n",
    "    B = torch.linspace(-1, 1, n_b)\n",
    "\n",
    "    objective = torch.zeros((n_w, n_b))\n",
    "    for i, w in enumerate(W):\n",
    "        for j, b in enumerate(B):\n",
    "            y_hat = w * X_train + b\n",
    "            objective[i, j] = np.log(loss_fn(y_train, y_hat).item())\n",
    "    plt.contourf(W, B, objective.T)\n",
    "\n",
    "    # Plot the iteration\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.train\n",
    "    res = []\n",
    "    for epoch in range(100):\n",
    "        if epoch % 10 == 0:\n",
    "            plt.plot(model.linear_layer.weight.data, model.linear_layer.bias.data, \"*r\")\n",
    "        y_hat = model(X_train)\n",
    "        loss = loss_fn(y_train, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    plt.plot(np.asarray(res), \"r*\")\n",
    "\n",
    "    # Plot optimal solution\n",
    "    skmodel = LR().fit(X_train, y_train)\n",
    "    plt.plot(skmodel.coef_, skmodel.intercept_, \"b*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde39296-a6e1-4eec-9820-68d4fb933a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "model = LinearModel()\n",
    "plot_convergence(x_train, y_train, model, loss_fn, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c663b-8ce3-4453-85a0-ea7d89322862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
