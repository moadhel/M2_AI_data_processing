{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018b7b0-b1c7-4000-b95b-9e15070bf52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4a648-9ba2-44dc-b271-5a25a99efa1f",
   "metadata": {},
   "source": [
    "# Download the data\n",
    "_This notebook is mostly taken from https://github.com/rasbt/machine-learning-book/tree/main, chapter 15._\n",
    "\n",
    "We are going to download from the Project Gutenberg website (https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875de34-239c-4456-b3ad-41c2fd1e347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "path_to_save_the_file = \"../Data/pg5711.txt\"\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://www.gutenberg.org/cache/epub/5711/pg5711.txt\", path_to_save_the_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2763f-461d-4dbe-8fc5-ba7e35ae81f2",
   "metadata": {},
   "source": [
    "The whole file can be read and store in the _text_ variable using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d30cf-4a21-4d56-a5fa-5a4d980d60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_save_the_file, \"r\", encoding=\"utf8\") as the_file:\n",
    "    text = the_file.read()\n",
    "print(text[0:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf6da1-cb97-49f3-b878-a26443f15567",
   "metadata": {},
   "source": [
    "We can remove some parts of the text not relevant to our analysis (beginning and ending):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b642e70-cd8d-440c-b13f-45eed11f3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = text.find(\"Première Partie\")\n",
    "end_index = text.find(\"*** END OF THE PROJECT GUTENBERG EBOOK GERMINAL ***\")\n",
    "text = text[start_index:end_index]\n",
    "print(f\"Size of the book (as the number of characters): {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780939ec-fc7c-45f5-8cfd-e1d172756398",
   "metadata": {},
   "source": [
    "# Prepare the tokenizer\n",
    "For this labwork, we are going to do simple things: a token will be a character from the book. So the first steps to prepare the data is to:\n",
    "1. Get the set of unique characters of the string _text_\n",
    "2. Create a function converting each caracters into an unique value\n",
    "3. For later inference, create the inverse function\n",
    "\n",
    "This can be done using the ScikitLearn _LabelEncoder_ function. \n",
    "\n",
    "Hint: to convert each caracter of a long string to an numpy array of caracters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1c493-89d9-4aa9-8e3f-40e580e83b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = np.array(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a2089-b019-4e47-9ef5-789c09afac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad6781-e583-4832-bc72-c3b405641fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the whole text -> the variable should be named: text_encoded\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993ced8-ea74-4f48-aa8e-d8c20b018f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{text[:8]} == Encoding ==> {text_encoded[:8]}\")\n",
    "print(\n",
    "    f\"{text_encoded[23:37]} == Decoding ==> {le.inverse_transform(text_encoded[23:37])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c7e36-7ccc-4d80-b4eb-7b84985a1dda",
   "metadata": {},
   "source": [
    "# Torch Data Loader\n",
    "For this learning problem, we want to learn the most probable caracters given a sequence of caracters of fixed lenght. To build the data set, we need to extract such sequences. The size of the sequence is a very important hyperparameter: too small, we would not capture only local dependences, and too big, it will results in a too complicated problem. It will be set to 40 here. So we want to predict the 41th caracters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcf695-d4bc-48ee-becf-4b6eff3c9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "chunks = np.array(\n",
    "    [text_encoded[i : i + chunk_size] for i in range(len(text_encoded) - seq_length)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3713a7b-e89b-45a6-ae0d-dcc45e7ec756",
   "metadata": {},
   "source": [
    "The torch dataset should output the first 40 caracters of the chunck as the input variable and the last 40 caracter as the output variable. To compute the BTT we need to feed the model with a caracter and its immediate next one of the sequence until predicting the 41th ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc1d76-d5ea-4805-b965-a22f5e7a397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        return chunk[:-1].long(), chunk[1:].long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7474fe-8e4c-4683-bb07-4f93c8ff1829",
   "metadata": {},
   "source": [
    "And now we are done with the Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41beee8c-56f2-4cc5-b2d6-5c64c908069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dataset = TextDataset(torch.tensor(chunks))\n",
    "torch.manual_seed(1)\n",
    "batch_size = 512\n",
    "seq_data_loader = DataLoader(\n",
    "    seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a048c-e736-4037-bf57-2bcda06f0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "(seq, target) = next(iter(seq_dataset))\n",
    "print(\"Sequence:\\n\", \"\".join(le.inverse_transform(seq)))\n",
    "print(\"Target:\\n\", \"\".join(le.inverse_transform(target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d5945-69d4-4309-b99f-c54fb9040014",
   "metadata": {},
   "source": [
    "# RNN Model\n",
    "We miss a last step to build the model: the tokens are integer number that corresponds to unique words. We need to convert this values into an input feature vector. One hot encoding can be used as for class label. However, it will result in very sparse feature and the associated feature space will be mostly empty. Something which is difficult to work with.\n",
    "A more appropriate approach is to map each token into a vector space of fixed size. For instance, $\\{1, 2, 3\\}$ could be mapped to $[(0,1), (1,0), (1,1)]$ in $\\mathbb{R}^2$. This is called _embedding_ and as usuall with deep learning, it can be learned from the data once the embedding dimension is selected. A rule of dumb is that the embedding dimensions to much smaller than the number of possible outcomes for the token values. Some work mention a square root relation. Should be tested in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8eaddd-50a7-47cc-a88d-4cd9407aa98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, token_size: int, embed_dim: int, rnn_hidden_size: int):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(token_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, token_size)\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # PASS\n",
    "        # RNN is waiting for [n_batch, n_layer, input_size = embded_dim]\n",
    "        # We need to add one dimension for the number of lstm layers - here 1\n",
    "        out = self.embedding(X).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        # out for rnn/lstm  is of size [n_batch, n_layers, rnn_hidden_size] with n_layers 1\n",
    "        # Here we \"remove\" the dimension corresponding to the layers == view(n_batch, token_size)\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden_layers(\n",
    "        self, batch_size, device=\"cpu\"\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # LSTM waits for input of size (n_layers, n_batch, rnn_hidden_size)\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6a630-5bfe-4cb8-b425-344c3217d4e7",
   "metadata": {},
   "source": [
    "We can defined the learning step. For this labwork, we don't define a test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a5730-7925-4bd7-9cab-9e625a774144",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size = le.classes_.size\n",
    "embed_dim = 32\n",
    "rnn_hidden_size = 512\n",
    "torch.manual_seed(1)\n",
    "model = RNN(token_size, embed_dim, rnn_hidden_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f2c6b-e6ab-49be-9f6d-3090bcbace10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device - RNN is training is very long -> A GPU would be very welcome\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    accu = 0.0\n",
    "    for seq_batch, target_batch in seq_data_loader:\n",
    "        seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "        loss = 0.0\n",
    "        # Implement BTT\n",
    "        pass        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accu += loss.item()\n",
    "    print(f\"Epoch {epoch}: {accu/len(seq_data_loader)/seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eba84a-b47a-49fe-aab9-ce89f9e38a6d",
   "metadata": {},
   "source": [
    "Once the model is learned, it possible to get the most probable character given the (sequence of) previous one, as for classification we only need to get the caracter with the maximum output. However, it is possible to take into account the most probables caracters by looking at the multinomial distribution, with parameters provided by the model outputs, and generate caracters according to the distribution. Lets consider the multinomial distribution with parameters $[0.45,0.40,0.15]$: the most probable class is the first one. But using pytorch distribution facilities, it is possible to sample class such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899b9b4-5eb9-424c-af0f-669e4f67bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "scale = 1\n",
    "dist = Categorical(probs=torch.tensor([0.45, 0.40, 0.15]) ** scale)\n",
    "dist.sample([20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0a3e3-931a-471b-abf7-0b6013c6cf93",
   "metadata": {},
   "source": [
    "The _scale_ parameter is used to control the degree of randomness for the sampling of randomness. Higher value of scale will results in more \"one-hot\" like sample vector. For two almost equally probable classes, a higher value of _scale_ will favor the most probable class, while a lower value of scale will make both (or all) classes equally probable during the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99c2a9-c75b-4fc4-aaa9-719c8ba8188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, sharey=True, sharex=True, figsize=(15, 5))\n",
    "for scale, ax in zip([0.1, 1, 10], axs):\n",
    "    dist = Categorical(probs=torch.tensor([0.45, 0.40, 0.15]) ** scale)\n",
    "    samples = dist.sample([10000])\n",
    "    ax.hist(samples, bins=[-0.5, 0.5, 1.5, 2.5], density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6f832-2515-470f-bf65-042b5df9119b",
   "metadata": {},
   "source": [
    "We can now define the sampling strategy to generate a text, given some sentence. We will use the scale parameter to control the amount of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096e064-9763-4222-a84d-b3e2f931d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sampling(\n",
    "    model: RNN,\n",
    "    le: LabelEncoder,\n",
    "    starting_str: str,\n",
    "    generated_text_len: int = 500,\n",
    "    scale_factor: float = 1.0,\n",
    "    random_state=0,\n",
    "):\n",
    "    torch.manual_seed(random_state)\n",
    "    generated_str = starting_str\n",
    "    # Encode input\n",
    "    encoded_input = torch.tensor(le.transform(list(starting_str))).reshape(1, -1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.init_hidden_layers(1)\n",
    "        # Iterate over the input to initialize h and c\n",
    "        for c in range(len(starting_str)):\n",
    "            _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
    "\n",
    "        # Iterate until the generated text has the correct size\n",
    "        last_char = encoded_input[:, -1]\n",
    "        for i in range(generated_text_len):\n",
    "            logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "            logits = scale_factor * torch.squeeze(logits, 0)\n",
    "            m = Categorical(logits=logits)\n",
    "            last_char = m.sample()\n",
    "            generated_str += le.classes_[last_char.item()]\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b36c9-1f4c-4758-830e-5cb488e82a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : load pre-trained model\n",
    "pass\n",
    "starting_str = \"les ouvriers crèvent\"\n",
    "print(model_sampling(model, le, starting_str, scale_factor=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
