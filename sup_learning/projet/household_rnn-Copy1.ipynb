{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18af70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data/text', <http.client.HTTPMessage at 0x28fcdaa85d0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "path_to_save_the_file = \"Data/text\"\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://www.gutenberg.org/cache/epub/5711/pg5711.txt\", path_to_save_the_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd29bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Germinal\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Ger...\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_save_the_file, \"r\", encoding=\"utf8\") as the_file:\n",
    "    text = the_file.read()\n",
    "print(text[0:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eba5363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the book (as the number of characters): 1011350\n",
      "[35 63 50 ...  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "start_index = text.find(\"Première Partie\")\n",
    "end_index = text.find(\"*** END OF THE PROJECT GUTENBERG EBOOK GERMINAL ***\")\n",
    "text = text[start_index:end_index]\n",
    "print(f\"Size of the book (as the number of characters): {len(text)}\")\n",
    "text_array = np.array(list(text))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(text_array)\n",
    "text_encoded=le.transform(text_array)\n",
    "print(text_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e06a0250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels=[]\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequence = data[i+seq_length:i:-1]\n",
    "        sequences.append(sequence)\n",
    "        labels.append(data[i+seq_length])\n",
    "    return np.array(sequences),np.array(labels)\n",
    "\n",
    "seq_length =10 \n",
    "\n",
    "sequences,labels = create_sequences(text_encoded, seq_length)\n",
    "\n",
    "train_val_size = int(0.8 * len(sequences))\n",
    "X_train_val, y_train_val = sequences[:train_val_size],labels[:train_val_size]\n",
    "X_test,y_test = sequences[train_val_size:],labels[train_val_size:]\n",
    "\n",
    "train_size = int(0.8 * len(X_train_val))\n",
    "X_train,y_train=sequences[:train_size],labels[:train_size]\n",
    "X_val,y_val=sequences[train_size:train_val_size],labels[train_size:train_val_size]\n",
    "\n",
    "X_train,y_train = torch.tensor(X_train),torch.tensor(y_train)\n",
    "X_val,y_val = torch.tensor(X_val),torch.tensor(y_val)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "batch_size = 32  \n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab450b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, token_size: int, embed_dim: int, rnn_hidden_size: int,num_stack_layers:int):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(token_size, embed_dim)\n",
    "        self.num_stack_layers=num_stack_layers\n",
    "\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True,num_layers=num_stack_layers)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, h: torch.Tensor, c: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        out = self.embedding(X).squeeze(1)\n",
    "        out,(h,c)= self.rnn(out, (h, c))\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out,h,c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 4653.509662650429\n",
      "Test Loss: 2080.4442312989054\n",
      "Epoch [2/30], Loss: 4650.185901908531\n",
      "Test Loss: 1644.7905549882335\n",
      "Epoch [3/30], Loss: 4650.044820674812\n",
      "Test Loss: 1225.0638917488388\n",
      "Epoch [4/30], Loss: 4650.140255711051\n",
      "Test Loss: 812.2780590057373\n",
      "Epoch [5/30], Loss: 4650.0000203993695\n",
      "Test Loss: 642.9327829880051\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "input_size = 1 \n",
    "hidden_size = 32  \n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "num_stack_layers=2\n",
    "token_size = le.classes_.size\n",
    "embed_dim = 32\n",
    "\n",
    "\n",
    "\n",
    "model = RNN(token_size,embed_dim, hidden_size,num_stack_layers)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Entraînement du modèle\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0 \n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels=inputs.to(device), labels.to(device)\n",
    "        h0=torch.zeros(num_stack_layers,inputs.size(0),hidden_size)\n",
    "        c0=torch.zeros(num_stack_layers,inputs.size(0),hidden_size)\n",
    "        h0,c0 = h0.to(device),c0.to(device)\n",
    "\n",
    "        output,_,_=model(inputs,h0,c0)\n",
    "\n",
    "\n",
    "        loss=loss_fn(output.squeeze(1),labels.float())\n",
    "        total_loss+=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()            \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Évaluation du modèle\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels=inputs.to(device), labels.to(device)\n",
    "            h0=torch.zeros(num_stack_layers,inputs.size(0),hidden_size)\n",
    "            c0=torch.zeros(num_stack_layers,inputs.size(0),hidden_size)\n",
    "            h0,c0 = h0.to(device),c0.to(device)\n",
    "            output,_,_=model(inputs,h0,c0)\n",
    "            loss=criterion(output,labels)\n",
    "            test_loss+=loss.item()\n",
    "        print(f'Test Loss: {test_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54802831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_std=model(X_test_std).numpy()\n",
    "    predicted_mean=model(X_test_mean).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
